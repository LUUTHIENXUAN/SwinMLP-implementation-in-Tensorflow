# SwinMLP-implementation-in-Tensorflow

Swin MLP, which is an adaption of Swin Transformer by replacing all multi-head self-attention (MHSA) blocks by MLP layers (more precisely it is a group linear layer). The shifted window configuration can also significantly improve the performance of vanilla MLP architectures.

Official implementation in Pytorch
https://github.com/microsoft/Swin-Transformer/tree/5d2aede42b4b12cb0e7a2448b58820aeda604426
